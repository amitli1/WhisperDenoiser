{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e615cd2",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid green; padding: 20px;\">\n",
    "  <p style='color:green'> Notes & Qs:\n",
    "      <li style='color:green'> Batch size 16 (with tiny whisper) - 47/48G </li>            \n",
    "      <li style='color:green'> Batch size 8 (with medium whisper) - 28/48G </li>            \n",
    "      <li style='color:green'> why there are 2 loss ? (l1 and cross-entory) ? </li>            \n",
    "  </p>  \n",
    "  <p style='color:red'> Todo:\n",
    "      <li style='color:red'> Add normalizer (ru norm) </li>                        \n",
    "  </p>  \n",
    "  <p style='color:blue'> Conclusions & Notes:      \n",
    "      <li style='color:blue'> ? </li>      \n",
    "  </p>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f40aa-2767-4eda-ba38-a2bce698379e",
   "metadata": {},
   "source": [
    "![title](\"./full_arc.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbc6fb-cd36-4196-8e7b-e52541f97815",
   "metadata": {},
   "source": [
    "![title](\"img/picture.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec701f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch           import Trainer\n",
    "from typing                      import Optional, List, Tuple, Union\n",
    "from whisper.tokenizer           import get_tokenizer\n",
    "from lightning.pytorch           import Trainer\n",
    "from torch.nn.functional         import pad\n",
    "from pytorch_lightning.loggers   import TensorBoardLogger\n",
    "from pytorch_lightning.loggers   import WandbLogger\n",
    "from sklearn.model_selection     import train_test_split\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from tqdm                        import tqdm\n",
    "\n",
    "import torchaudio.transforms     as at\n",
    "import torch.nn                  as nn\n",
    "import pandas                    as pd\n",
    "import numpy                     as np\n",
    "import lightning                 as pl\n",
    "import torch.nn.functional       as f\n",
    "\n",
    "import whisper\n",
    "import evaluate\n",
    "import torch\n",
    "import torchaudio\n",
    "import glob\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e332c22-aad4-409e-ac89-32bf6f6f571d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE    = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a9851",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='toc'> <center> Table Of Content: </center> </a>  </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f1010",
   "metadata": {},
   "source": [
    "[DATASET](#DATASET) <br>\n",
    "[UNET Model](#unet_model) <br>\n",
    "[Whisper Denoiser](#whisper_denoiser) <br>\n",
    "[Train](#train) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53b34c-cbab-4d94-b6be-8e34dc9939a8",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:red;\"> <a id='params'> <center> PARAMS: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7c422a-29ee-4adb-a5ff-7d8df6dfef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_TYPE = \"base\"\n",
    "LANGUAGE     = \"ru\"\n",
    "BATCH_SIZE   = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc796974",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='DATASET'> <center> DATASET: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b908c6",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023301de-06d8-4714-8f5a-0a16c01ddae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df                = pd.read_csv(\"noised_files_v2.csv\")\n",
    "train_df, temp_df = train_test_split(df,      test_size=0.3, random_state=42)\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2bb6fe-e297-4db5-99ee-0c6fc45c2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df), len(val_df) # (3500, 1005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "627c166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_data, language):\n",
    "        \n",
    "        self.df                     = df_data          \n",
    "        self.woptions               = whisper.DecodingOptions(language=language, without_timestamps=True)        \n",
    "        self.multilingual_tokenizer = get_tokenizer(multilingual=True, language=language, task=self.woptions.task)\n",
    "\n",
    "        \n",
    "    def get_sentance(self, fname):                \n",
    "        fname    = fname[fname.rfind('/') + 1 : ]        \n",
    "        fname    = fname.replace(\"wav\", \"mp3\")        \n",
    "        sentence = self.df[self.df.file == fname]['sentence'].values[0]             \n",
    "        \n",
    "        return sentence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, i):               \n",
    "        fname               = self.df.file.values[i]  \n",
    "        sentance            = self.get_sentance(fname)        \n",
    "        \n",
    "        noise_file_name     = rf\"Noised_Dataset/{fname.replace('mp3', 'wav')}\"                     \n",
    "        audio, sr           = torchaudio.load(noise_file_name, normalize=True)        \n",
    "        audio               = whisper.pad_or_trim(audio.flatten())        \n",
    "        noise_mels          = whisper.log_mel_spectrogram(audio).squeeze()                \n",
    "\n",
    "        clean_file_name     = rf\"Clean_Dataset/{fname.replace('mp3', 'wav')}\"          \n",
    "        audio, sr           = torchaudio.load(clean_file_name, normalize=True)        \n",
    "        audio               = whisper.pad_or_trim(audio.flatten())\n",
    "        clean_mels          = whisper.log_mel_spectrogram(audio).squeeze()        \n",
    "\n",
    "        # --- get tokens (will be used for ce loss)\n",
    "        multilingual_tokens = [*self.multilingual_tokenizer.sot_sequence_including_notimestamps] + self.multilingual_tokenizer.encode(sentance)\n",
    "        gt_tokens           = multilingual_tokens[1:] + [self.multilingual_tokenizer.eot]\n",
    "              \n",
    "        return {\"file_name\"          :fname,\n",
    "                \"clean_mels\"         : clean_mels.unsqueeze(dim=0),   \n",
    "                \"noise_mels\"         : noise_mels.unsqueeze(dim=0), \n",
    "                \"sentance\"           : sentance,\n",
    "                \"multilingual_tokens\": multilingual_tokens,\n",
    "                \"gt_tokens\"          : gt_tokens}\n",
    "        \n",
    "\n",
    "\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    def __call__(self, features):\n",
    "        \n",
    "        file_name, clean_mels, noise_mels, sentance, multilingual_tokens, gt_tokens = [], [], [], [], [], []\n",
    "        for f in features:\n",
    "            file_name            .append(f[\"file_name\"])\n",
    "            clean_mels           .append(f[\"clean_mels\"])            \n",
    "            noise_mels           .append(f[\"noise_mels\"])            \n",
    "            sentance             .append(f[\"sentance\"]) \n",
    "            multilingual_tokens  .append(f[\"multilingual_tokens\"])\n",
    "            gt_tokens            .append(f[\"gt_tokens\"])\n",
    "\n",
    "        clean_mels = torch.concat([mel[None, :] for mel in clean_mels])\n",
    "        noise_mels = torch.concat([mel[None, :] for mel in noise_mels])\n",
    "\n",
    "        gt_tokens_lengths          = [len(lab) for lab in gt_tokens]\n",
    "        multilingual_tokens_length = [len(e) for e in multilingual_tokens]\n",
    "        max_label_len              = max(gt_tokens_lengths + multilingual_tokens_length)\n",
    "\n",
    "        gt_tokens           = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100)     for lab, lab_len in zip(gt_tokens,           gt_tokens_lengths)]\n",
    "        multilingual_tokens = [np.pad(e,   (0, max_label_len - e_len),   'constant', constant_values=50257)    for e, e_len     in zip(multilingual_tokens, multilingual_tokens_length)] \n",
    "                        \n",
    "        batch = {            \n",
    "            \"multilingual_tokens\":    multilingual_tokens,\n",
    "            \"gt_tokens\"          :    gt_tokens \n",
    "        }\n",
    "\n",
    "        batch               = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
    "        batch[\"file_name\"]  = file_name\n",
    "        batch[\"sentance\"]   = sentance\n",
    "        batch[\"clean_mels\"] = clean_mels\n",
    "        batch[\"noise_mels\"] = noise_mels\n",
    "                                    \n",
    "        return batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cd60307-b2a4-4994-8c8c-8889cddc6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset    = AudioDataset(train_df, LANGUAGE)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size = BATCH_SIZE, \n",
    "                                               drop_last  = True, \n",
    "                                               shuffle    = True,\n",
    "                                               collate_fn = WhisperDataCollatorWithPadding())\n",
    "\n",
    "val_dataset      = AudioDataset(val_df, LANGUAGE)\n",
    "val_dataloader   = torch.utils.data.DataLoader(val_dataset, \n",
    "                                               batch_size = BATCH_SIZE, \n",
    "                                               drop_last  = True, \n",
    "                                               shuffle    = True,\n",
    "                                               collate_fn = WhisperDataCollatorWithPadding())\n",
    "\n",
    "\n",
    "res = next(iter(train_dataloader))\n",
    "#type(res['multilingual_tokens'][0])\n",
    "#type(res), res.keys(), res['clean_mels'].shape, res['noise_mels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081fb16-85bc-4f77-887b-c56bb1131113",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset     = AudioDataset(test_df, LANGUAGE)\n",
    "test_dataloader  = torch.utils.data.DataLoader(test_dataset, \n",
    "                                               batch_size = BATCH_SIZE, \n",
    "                                               drop_last  = True, \n",
    "                                               shuffle    = True,\n",
    "                                               collate_fn = WhisperDataCollatorWithPadding())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2a0d4",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='unet_model'> <center> Unet Model: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa246d",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc76a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\" A single Res-Block module \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, use_bias: bool):\n",
    "        \"\"\"\n",
    "        Init\n",
    "        :param dim: The dimension\n",
    "        :param use_bias: Flag to use bias or not\n",
    "        \"\"\"\n",
    "        super(ResnetBlock, self).__init__()\n",
    "\n",
    "        # A res-block without the skip-connection, pad-conv-norm-relu-pad-conv-norm\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(dim, dim // 4, kernel_size=1, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(dim // 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(dim // 4, dim // 4, kernel_size=3, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(dim // 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(dim // 4, dim, kernel_size=1, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Feed forward run\n",
    "        :param input_tensor: The input tensor\n",
    "        :return: The output tensor\n",
    "        \"\"\"\n",
    "        # The skip connection is applied here\n",
    "        return input_tensor + self.conv_block(input_tensor)\n",
    "class RescaleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Rescale Block class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers: int, scale: Optional[float] = 0.5, n_mels: Optional[int] = 64,\n",
    "                 use_bias: Optional[bool] = True):\n",
    "        \"\"\"\n",
    "        Init\n",
    "        :param n_layers: The number of layers\n",
    "        :param scale: Scale factor\n",
    "        :param n_mels: Base number of channels\n",
    "        :param use_bias: Flag to use bias or not\n",
    "        \"\"\"\n",
    "        super(RescaleBlock, self).__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.conv_layers = [None] * n_layers\n",
    "\n",
    "        in_channel_power = scale > 1\n",
    "        out_channel_power = scale < 1\n",
    "        i_range = range(n_layers) if scale < 1 else range(n_layers - 1, -1, -1)\n",
    "\n",
    "        for i in i_range:\n",
    "            self.conv_layers[i] = nn.Sequential(\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=n_mels * 2 ** (i + in_channel_power),\n",
    "                        out_channels=n_mels * 2 ** (i + out_channel_power),\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        bias=use_bias,\n",
    "                    )\n",
    "                ),\n",
    "                nn.BatchNorm2d(n_mels * 2 ** (i + out_channel_power)),\n",
    "                nn.LeakyReLU(0.2, True))\n",
    "\n",
    "            self.add_module(\"conv_%d\" % i, self.conv_layers[i])\n",
    "\n",
    "        if scale > 1:\n",
    "            self.conv_layers = self.conv_layers[::-1]\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor,\n",
    "                pyramid: Optional[torch.Tensor] = None,\n",
    "                return_all_scales: Optional[bool] = False,\n",
    "                skip: Optional[bool] = False) -> Tuple[torch.Tensor, Optional[List[torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_tensor: The input tensor\n",
    "        :param pyramid: The pyramid tensor\n",
    "        :param return_all_scales: Flag to return all scales\n",
    "        :param skip: Flag to skip or not\n",
    "        :return: Tuple with feature maps and all scales (if return_all_scales is True)\n",
    "        \"\"\"\n",
    "        feature_map = input_tensor\n",
    "        all_scales = []\n",
    "        if return_all_scales:\n",
    "            all_scales.append(feature_map)\n",
    "\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "\n",
    "            if self.scale > 1.0:\n",
    "                feature_map = f.interpolate(\n",
    "                    feature_map, scale_factor=self.scale, mode=\"nearest\"\n",
    "                )\n",
    "\n",
    "            feature_map = conv_layer(feature_map)\n",
    "\n",
    "            if skip:\n",
    "                feature_map = feature_map + pyramid[-i - 2]\n",
    "\n",
    "            if self.scale < 1.0:\n",
    "                feature_map = self.max_pool(feature_map)\n",
    "\n",
    "            if return_all_scales:\n",
    "                all_scales.append(feature_map)\n",
    "\n",
    "        return (feature_map, all_scales) if return_all_scales else (feature_map, None)\n",
    "    \n",
    "class Unet(nn.Module):\n",
    "    \"\"\" Architecture of the Unet, uses res-blocks \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_mels: Optional[int] = 64,\n",
    "            n_blocks: Optional[int] = 6,\n",
    "            n_downsampling: Optional[int] = 3,\n",
    "            use_bias: Optional[bool] = True,\n",
    "            skip_flag: Optional[bool] = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Init\n",
    "        :param n_mels: The base number of channels\n",
    "        :param n_blocks: The number of res blocks\n",
    "        :param n_downsampling: The number of downsampling blocks\n",
    "        :param use_bias: Use bias or not\n",
    "        :param skip_flag: Use skip connections or not\n",
    "        \"\"\"\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        # Determine whether to use skip connections\n",
    "        self.skip = skip_flag\n",
    "\n",
    "        # Entry block\n",
    "        # First conv-block, no stride so image dims are kept and channels dim is expanded (pad-conv-norm-relu)\n",
    "        self.entry_block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(1, n_mels, kernel_size=7, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(n_mels),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        # Downscaling\n",
    "        # A sequence of strided conv-blocks. Image dims shrink by 2, channels dim expands by 2 at each block\n",
    "        self.downscale_block = RescaleBlock(n_downsampling, 0.5, n_mels, True)\n",
    "\n",
    "        # Bottleneck\n",
    "        # A sequence of res-blocks\n",
    "        bottleneck_block = []\n",
    "        for _ in range(n_blocks):\n",
    "            # noinspection PyUnboundLocalVariable\n",
    "            bottleneck_block += [\n",
    "                ResnetBlock(n_mels * 2 ** n_downsampling, use_bias=use_bias)\n",
    "            ]\n",
    "        self.bottleneck_block = nn.Sequential(*bottleneck_block)\n",
    "\n",
    "        # Upscaling\n",
    "        # A sequence of transposed-conv-blocks, Image dims expand by 2, channels dim shrinks by 2 at each block\\\n",
    "        self.upscale_block = RescaleBlock(n_downsampling, 2.0, n_mels, True)\n",
    "\n",
    "        # Final block\n",
    "        # No stride so image dims are kept and channels dim shrinks to 3 (output image channels)\n",
    "        self.final_block = nn.Sequential(\n",
    "            # nn.ReflectionPad2d(3), nn.Conv2d(n_mels, 1, kernel_size=7), nn.Tanh()\n",
    "            # TODO: without Tanh, for not having output [-1,1]\n",
    "            nn.ReflectionPad2d(3), nn.Conv2d(n_mels, 1, kernel_size=7)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Feed forward run\n",
    "        :param input_tensor: The input Tensor\n",
    "        :param output_size: The output size\n",
    "        :param random_affine: List of random affine numbers\n",
    "        :return: The output tensor\n",
    "        \"\"\"\n",
    "        # A condition for having the output at same size as the scaled input is having even output_size\n",
    "\n",
    "        # Entry block\n",
    "        feature_map = self.entry_block(input_tensor)\n",
    "\n",
    "        # Downscale block\n",
    "        feature_map, downscales = self.downscale_block(\n",
    "            feature_map, return_all_scales=self.skip\n",
    "        )\n",
    "\n",
    "        # Bottleneck (res-blocks)\n",
    "        feature_map = self.bottleneck_block(feature_map)\n",
    "\n",
    "        # Upscale block\n",
    "        feature_map, _ = self.upscale_block(\n",
    "            feature_map, pyramid=downscales, skip=self.skip\n",
    "        )\n",
    "\n",
    "        # Final block\n",
    "        output_tensor = self.final_block(feature_map)\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        cuda  = True\n",
    "        state = { 'net': self.state_dict() if cuda else self.state_dict() }\n",
    "\n",
    "        torch.save(state,  model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f7c6c",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='whisper_denoiser'> <center> Whisper Denoiser: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c57418",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5ef874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetWhisperModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        lang                 = LANGUAGE\n",
    "        self.loss_ce         = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.l1loss          = torch.nn.L1Loss()\n",
    "        self.metrics_wer     = evaluate.load(\"wer\")\n",
    "        \n",
    "        self.whisper_options = whisper.DecodingOptions(language=lang, without_timestamps=True)\n",
    "        self.whisper_model   = whisper.load_model(WHISPER_TYPE)\n",
    "        self.tokenizer       = whisper.tokenizer.get_tokenizer(True, language=LANGUAGE, task=self.whisper_options.task)\n",
    "\n",
    "        for param in self.whisper_model.parameters():\n",
    "            param.requires_grad = False\n",
    "  \n",
    "        config                   = {}\n",
    "        config[\"n_mels\"]         = 80\n",
    "        config[\"n_blocks\"]       = 6\n",
    "        config[\"n_downsampling\"] = 3\n",
    "        config[\"use_bias\"]       = True\n",
    "        config[\"skip_flag\"]      = True\n",
    "        self.unet                = Unet(config[\"n_mels\"], config[\"n_blocks\"], config[\"n_downsampling\"], config[\"use_bias\"], config[\"skip_flag\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        print(\"\\n\")\n",
    "        print(\"--> forward\")\n",
    "        logits       = self.unet(x)        \n",
    "        whisper_pred = self.whisper(logits)        \n",
    "        \n",
    "        return logits, whisper_pred\n",
    "\n",
    "    def calc_wer_and_loss(self, batch):\n",
    "                \n",
    "        clean_mels          = batch['clean_mels']        \n",
    "        noise_mels          = batch['noise_mels']        \n",
    "        gt_sentance         = batch['sentance']    \n",
    "        multilingual_tokens = batch['multilingual_tokens'].long()\n",
    "        gt_tokens           = batch['gt_tokens'].long() \n",
    "        \n",
    "        logits              = self.unet(noise_mels)                      \n",
    "        audio_features      = self.whisper_model.encoder(logits.squeeze())        \n",
    "        out                 = self.whisper_model.decoder(multilingual_tokens, audio_features)\n",
    "        \n",
    "        # --- calc loss        \n",
    "        l1_loss              = self.l1loss(logits, clean_mels)                   \n",
    "        ce_loss              = self.loss_ce(out.view(-1, out.size(-1)), gt_tokens.view(-1))                \n",
    "        loss                 = l1_loss * 0.5 + ce_loss * 0.5\n",
    "        #print(\"\\n\")\n",
    "        #print(f\"[l1_loss] type = {type(l1_loss)}, loss = {l1_loss}\")\n",
    "        #print(f\"[ce_loss] type = {type(ce_loss)}, loss = {ce_loss}\")\n",
    "        #print(f\"[loss]    type = {type(loss)}   , loss = {loss}\")\n",
    "\n",
    "        # --- calc WER        \n",
    "        out[out == -100]                         = self.tokenizer.eot\n",
    "        gt_tokens[gt_tokens == -100]             = self.tokenizer.eot\n",
    "        o_list, l_list                           = [], []\n",
    "        \n",
    "        for o, l in zip(out, gt_tokens):\n",
    "            o = torch.argmax(o, dim=1)\n",
    "            o_list.append(self.tokenizer.decode(o).lower())\n",
    "            l_list.append(self.tokenizer.decode(l).lower().replace('<|endoftext|>', '').replace(\"<|en|>\", \"\").replace(\"<|transcribe|>\", \"\").replace(\"<|notimestamps|>\", \"\"))                \n",
    "        wer = self.metrics_wer.compute(references=l_list, predictions=o_list)\n",
    "        \n",
    "        return ce_loss, wer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):        \n",
    "        loss, wer = self.calc_wer_and_loss(batch)    \n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train/wer\", wer,   on_step=True, on_epoch=True, prog_bar=True, logger=True)        \n",
    "        return {\"loss\":loss}\n",
    "\n",
    "\n",
    "    def eval_step(self, batch):        \n",
    "        loss, wer = self.calc_wer_and_loss(batch)            \n",
    "        self.log(\"val/loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/wer\",  wer,  on_epoch=True, prog_bar=True, logger=True)                    \n",
    "        return {\"loss\": loss, \"wer\": wer}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):                  \n",
    "        loss, wer = self.calc_wer_and_loss(batch)            \n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_wer\",  wer,  on_epoch=True, prog_bar=True, logger=True)                    \n",
    "        #return self.eval_step(batch)\n",
    "        return {\"val_loss\": loss, \"val_wer\": wer}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):         \n",
    "        loss, wer = self.calc_wer_and_loss(batch)            \n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_wer\",  wer,  on_epoch=True, prog_bar=True, logger=True)                    \n",
    "        #return self.eval_step(batch)\n",
    "        return {\"test_loss\": loss, \"test_wer\": wer}\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        optimizer  = torch.optim.Adam(parameters, lr=0.0001)        \n",
    "        return {\n",
    "                \"optimizer\": optimizer,\n",
    "                }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b184c69-77d7-4ef1-a8e5-e777d68397f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = UnetWhisperModel()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0446a-a6a6-45b5-8a5f-51117f311a29",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#16755D;\"> <a id='Debug_Code'> <center> Debug Code: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d80cc-c8a1-467f-a722-b88104e88678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30adb293-203f-4399-a1dc-f6929832e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_CODE = False\n",
    "if DEBUG_CODE is True:\n",
    "\n",
    "    # --- define unet\n",
    "    config                   = {}\n",
    "    config[\"n_mels\"]         = 80\n",
    "    config[\"n_blocks\"]       = 6\n",
    "    config[\"n_downsampling\"] = 3\n",
    "    config[\"use_bias\"]       = True\n",
    "    config[\"skip_flag\"]      = True\n",
    "    unet                     = Unet(config[\"n_mels\"], config[\"n_blocks\"], config[\"n_downsampling\"], config[\"use_bias\"], config[\"skip_flag\"])\n",
    "\n",
    "    # --- get batch\n",
    "    batch                   = next(iter(train_dataloader))\n",
    "    clean_mels              = batch['clean_mels']        \n",
    "    noise_mels              = batch['noise_mels']   \n",
    "    gt_sentance             = batch['sentance']        \n",
    "    multilingual_tokens     = batch['multilingual_tokens'].long()\n",
    "    tokens_with_eot         = batch['tokens_with_eot'].long() \n",
    "        \n",
    "\n",
    "    # run unet    \n",
    "    logits          = unet(noise_mels)         \n",
    "    \n",
    "    # run whisper\n",
    "    whisper_options = whisper.DecodingOptions(language=LANGUAGE, without_timestamps=True)\n",
    "    whisper_model   = whisper.load_model(WHISPER_TYPE)\n",
    "\n",
    "    audio_features  = whisper_model.encoder(logits.squeeze().cuda())\n",
    "    out             = whisper_model.decoder(multilingual_tokens.cuda(), audio_features)\n",
    "\n",
    "    # calc loss\n",
    "    whisper_options = whisper.DecodingOptions(language=LANGUAGE, without_timestamps=True)\n",
    "    tokenizer       = whisper.tokenizer.get_tokenizer(True, language=LANGUAGE, task=whisper_options.task)\n",
    "    loss_ce         = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    l1loss          = torch.nn.L1Loss()\n",
    "    metrics_wer     = evaluate.load(\"wer\")\n",
    "    \n",
    "    l1_loss              = l1loss(logits, clean_mels)           \n",
    "    ce_loss              = loss_ce(out.view(-1, out.size(-1)).cuda(), tokens_with_eot.view(-1).cuda())        \n",
    "    loss                 = l1_loss * 0.5 + ce_loss * 0.5\n",
    "    \n",
    "    # --- calc WER\n",
    "    out[out == -100]                = tokenizer.eot\n",
    "    tokens_with_eot[tokens_with_eot == -100] = tokenizer.eot\n",
    "    o_list, l_list                  = [], []\n",
    "    \n",
    "    for o, l in zip(out, tokens_with_eot):\n",
    "        o = torch.argmax(o, dim=1)\n",
    "        o_list.append(tokenizer.decode(o).lower())\n",
    "        l_list.append(tokenizer.decode(l).lower().replace('<|endoftext|>', '').replace(\"<|en|>\", \"\").replace(\"<|transcribe|>\", \"\").replace(\"<|notimestamps|>\", \"\"))        \n",
    "    wer = metrics_wer.compute(references=l_list, predictions=o_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58441ff-7e11-4b6b-997d-7aec6ff70178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcf74b-4202-45c6-a146-101960c9d1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a60118c0",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='train'> <center> Train: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4f93d",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7defcfc7-7a65-45d2-a487-c4fe92c9406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamitli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vscode_user/VoiceTeam/Amit/wandb/run-20240613_113737-k0t7b6p9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit/runs/k0t7b6p9' target=\"_blank\">pious-microwave-8</a></strong> to <a href='https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit' target=\"_blank\">https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit/runs/k0t7b6p9' target=\"_blank\">https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit/runs/k0t7b6p9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit/runs/k0t7b6p9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3eea67a220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5eb843a-e6f7-425b-beef-0adbddb55e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor    = \"val_wer\",\n",
    "    mode       = \"min\",       \n",
    "    dirpath    = \"checkpoints/\", \n",
    "    filename   = \"best_model-{epoch:02d}-{val_wer:.4f}\",\n",
    "    save_top_k = 1,      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7455ffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | loss_ce       | CrossEntropyLoss | 0     \n",
      "1 | l1loss        | L1Loss           | 0     \n",
      "2 | whisper_model | Whisper          | 71.8 M\n",
      "3 | unet          | Unet             | 7.5 M \n",
      "---------------------------------------------------\n",
      "7.5 M     Trainable params\n",
      "71.8 M    Non-trainable params\n",
      "79.3 M    Total params\n",
      "317.224   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdff3c9a7cb4c4691c354003da9c65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[l1_loss] type = <class 'torch.Tensor'>, loss = nan\n",
      "[ce_loss] type = <class 'torch.Tensor'>, loss = nan\n",
      "[loss]    type = <class 'torch.Tensor'>   , loss = nan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokens_with_eot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m wandb_logger \u001b[38;5;241m=\u001b[39m WandbLogger(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnetWhisperModel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      3\u001b[0m trainer      \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs              \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      4\u001b[0m                           logger                  \u001b[38;5;241m=\u001b[39m wandb_logger,\n\u001b[1;32m      5\u001b[0m                           accumulate_grad_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      6\u001b[0m                           callbacks               \u001b[38;5;241m=\u001b[39m [checkpoint_callback])\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/lightning/pytorch/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 85\u001b[0m, in \u001b[0;36mUnetWhisperModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):                  \n\u001b[0;32m---> 85\u001b[0m     loss, wer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_wer_and_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m            \n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_wer\u001b[39m\u001b[38;5;124m\"\u001b[39m,  wer,  on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)                    \n",
      "Cell \u001b[0;32mIn[16], line 60\u001b[0m, in \u001b[0;36mUnetWhisperModel.calc_wer_and_loss\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# --- calc WER        \u001b[39;00m\n\u001b[1;32m     59\u001b[0m out[out \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]                         \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meot\n\u001b[0;32m---> 60\u001b[0m \u001b[43mtokens_with_eot\u001b[49m[tokens_with_eot \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meot\n\u001b[1;32m     61\u001b[0m o_list, l_list                           \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(out, tokens_with_eot):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens_with_eot' is not defined"
     ]
    }
   ],
   "source": [
    "#tb_logger    = TensorBoardLogger(save_dir=\"logs/\")\n",
    "wandb_logger = WandbLogger(project=\"UnetWhisperModel\") \n",
    "trainer      = pl.Trainer(max_epochs              = 50,\n",
    "                          logger                  = wandb_logger,\n",
    "                          accumulate_grad_batches = 4,\n",
    "                          callbacks               = [checkpoint_callback])\n",
    "\n",
    "trainer.fit(solver, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa752d-7e24-4c3e-b71b-518cffe3f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext tensorboard\n",
    "#%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56d906-708e-4ee7-a561-b36bf4509454",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='test_results'> <center> Test Results: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1366af0-dcba-44fe-8ff2-5c71697d60c1",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid green; padding: 20px;\">\n",
    "  <p style='color:green'>  Results: \n",
    "      <li style='color:green'> MEDIUM, pure whisper - 'test_base_results.csv' - WER= 0.416</li>                  \n",
    "      <li style='color:red'> MEDIUM, With UNET: - 'test_unet_results.csv' - WER= 0.482 - training was stop after 8 epochs </li>                  \n",
    "  </p>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501f250-66f9-4e0b-9665-3f1f8f6fbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model   = whisper.load_model(WHISPER_TYPE)\n",
    "whisper_options = whisper.DecodingOptions(language=\"ru\", without_timestamps=True)\n",
    "metrics_wer     = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b0cf8-8d29-40fb-9fc7-b45752b9726e",
   "metadata": {},
   "source": [
    "<h2> Base results: </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa6628-f1a2-4846-8614-8b3a0d60e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test_files = []\n",
    "l_results    = []\n",
    "l_gt         = []\n",
    "l_wer        = []\n",
    "\n",
    "for i in tqdm(range(len(test_df))):    \n",
    "    fname     = test_df.file.values[i]\n",
    "    gt        = test_df.sentence.values[i]\n",
    "    \n",
    "    full_name = rf\"Noised_Dataset/{fname.replace('mp3', 'wav')}\" \n",
    "    result    = whisper_model.transcribe(full_name)\n",
    "    result    = result[\"text\"]\n",
    "    wer_res   = metrics_wer.compute(references=[gt], predictions=[result])\n",
    "    \n",
    "    l_test_files .append(fname)    \n",
    "    l_results    .append(result)\n",
    "    l_gt         .append(gt)\n",
    "    l_wer        .append(wer_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2e1c2-efb1-4803-bb12-f079d6d33644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_results = pd.DataFrame({\"test_file\": l_test_files,\n",
    "                               \"prediction\": l_results,\n",
    "                               \"gt\"        : l_gt,\n",
    "                               \"wer\"       : l_wer})\n",
    "\n",
    "df_base_results.to_csv(\"test_base_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033f5c5-4b9e-4e2b-9f32-f94550facade",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_base_results.wer.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47b40d-ff49-42c6-b499-8ef3912681ca",
   "metadata": {},
   "source": [
    "<h2> With UNET: </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1f5a1-0ee4-40d1-80a8-d1f5e14aab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path  = \"checkpoints/best_model-epoch=07-val_wer=0.0000.ckpt\"\n",
    "unetWhisperModel = UnetWhisperModel.load_from_checkpoint(checkpoint_path)\n",
    "unetWhisperModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3622f-2b4b-4b1d-9822-bdfe915b8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test_files = []\n",
    "l_results    = []\n",
    "l_gt         = []\n",
    "l_wer        = []\n",
    "\n",
    "for i in tqdm(range(len(test_df))):    \n",
    "\n",
    "    # --- get file\n",
    "    fname           = test_df.file.values[i]\n",
    "    gt              = test_df.sentence.values[i]\n",
    "\n",
    "\n",
    "    # --- get spect\n",
    "    full_name       = rf\"Noised_Dataset/{fname.replace('mp3', 'wav')}\" \n",
    "    audio, sr       = torchaudio.load(full_name, normalize=True)        \n",
    "    audio           = whisper.pad_or_trim(audio.flatten())        \n",
    "    noise_mels      = whisper.log_mel_spectrogram(audio).squeeze()\n",
    "    noise_mels      = noise_mels.unsqueeze(dim=0) \t\t\n",
    "\n",
    "    # --- run unet\n",
    "    logits          = unetWhisperModel.unet(noise_mels.unsqueeze(1).cuda())\n",
    "    whisper_result  = whisper.decode(whisper_model, logits.squeeze(), whisper_options)        \n",
    "    result          = whisper_result.text\n",
    "\n",
    "    # --- calc WER\n",
    "    wer_res   = metrics_wer.compute(references=[gt], predictions=[result])\n",
    "\n",
    "    # --- add to lists\n",
    "    l_test_files .append(fname)    \n",
    "    l_results    .append(result)\n",
    "    l_gt         .append(gt)\n",
    "    l_wer        .append(wer_res)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80afbcfa-d9df-4342-bec3-31d61d55c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unet_results = pd.DataFrame({\"test_file\": l_test_files,\n",
    "                               \"prediction\": l_results,\n",
    "                               \"gt\"        : l_gt,\n",
    "                               \"wer\"       : l_wer})\n",
    "\n",
    "df_unet_results.to_csv(\"test_unet_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4567b-25be-4e8b-9aab-c51eefa51f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_unet_results.wer.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895bd41",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67175d-72dd-4803-8324-1ab0b2b7b059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
