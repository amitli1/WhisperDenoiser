{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e615cd2",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid green; padding: 20px;\">\n",
    "  <p style='color:green'> Notes & Qs:\n",
    "      <li style='color:green'> Batch size 16 (with tiny whisper) - 47/48G </li>            \n",
    "      <li style='color:green'> Batch size 8 (with medium whisper) - 28/48G </li>            \n",
    "      <li style='color:green'> why there are 2 loss ? (l1 and cross-entory) ? </li>            \n",
    "  </p>  \n",
    "  <p style='color:red'> Todo:\n",
    "      <li style='color:red'> Add normalizer (ru norm) </li>                        \n",
    "  </p>  \n",
    "  <p style='color:blue'> Conclusions & Notes:      \n",
    "      <li style='color:blue'> ? </li>      \n",
    "  </p>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f40aa-2767-4eda-ba38-a2bce698379e",
   "metadata": {},
   "source": [
    "![title](\"./full_arc.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbc6fb-cd36-4196-8e7b-e52541f97815",
   "metadata": {},
   "source": [
    "![title](\"img/picture.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deabaf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec701f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch           import Trainer\n",
    "from typing                      import Optional, List, Tuple, Union\n",
    "from whisper.tokenizer           import get_tokenizer\n",
    "from lightning.pytorch           import Trainer\n",
    "from torch.nn.functional         import pad\n",
    "from pytorch_lightning.loggers   import TensorBoardLogger\n",
    "from pytorch_lightning.loggers   import WandbLogger\n",
    "from sklearn.model_selection     import train_test_split\n",
    "\n",
    "import torchaudio.transforms     as at\n",
    "import torch.nn                  as nn\n",
    "import pandas                    as pd\n",
    "import numpy                     as np\n",
    "import lightning                 as pl\n",
    "import torch.nn.functional       as f\n",
    "\n",
    "import whisper\n",
    "import evaluate\n",
    "import torch\n",
    "import torchaudio\n",
    "import glob\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e332c22-aad4-409e-ac89-32bf6f6f571d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE    = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a9851",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='toc'> <center> Table Of Content: </center> </a>  </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f1010",
   "metadata": {},
   "source": [
    "[DATASET](#DATASET) <br>\n",
    "[UNET Model](#unet_model) <br>\n",
    "[Whisper Denoiser](#whisper_denoiser) <br>\n",
    "[Train](#train) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53b34c-cbab-4d94-b6be-8e34dc9939a8",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='params'> <center> PARAMS: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7c422a-29ee-4adb-a5ff-7d8df6dfef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_TYPE = \"medium\"\n",
    "BATCH_SIZE   = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc796974",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='DATASET'> <center> DATASET: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b908c6",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "023301de-06d8-4714-8f5a-0a16c01ddae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df                = pd.read_csv(\"noised_files.csv\")\n",
    "train_df, temp_df = train_test_split(df,      test_size=0.3, random_state=42)\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2bb6fe-e297-4db5-99ee-0c6fc45c2b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 1005)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "627c166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_data):\n",
    "        \n",
    "        self.df                     = df_data          \n",
    "        self.woptions               = whisper.DecodingOptions(language=\"ru\", without_timestamps=True)        \n",
    "\n",
    "        \n",
    "    def get_sentance(self, fname):                \n",
    "        fname    = fname[fname.rfind('/') + 1 : ]        \n",
    "        fname    = fname.replace(\"wav\", \"mp3\")        \n",
    "        sentence = self.df[self.df.file == fname]['sentence'].values[0]             \n",
    "        \n",
    "        return sentence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        #return 256\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):        \n",
    "        fname           = self.df.file.values[i]  \n",
    "        sentance        = self.get_sentance(fname)        \n",
    "        \n",
    "        noise_file_name = rf\"Noised_Dataset/{fname.replace('mp3', 'wav')}\"                     \n",
    "        audio, sr       = torchaudio.load(noise_file_name, normalize=True)        \n",
    "        audio           = whisper.pad_or_trim(audio.flatten())        \n",
    "        noise_mels      = whisper.log_mel_spectrogram(audio).squeeze()                \n",
    "\n",
    "        clean_file_name = rf\"Clean_Dataset/{fname.replace('mp3', 'wav')}\"          \n",
    "        audio, sr       = torchaudio.load(clean_file_name, normalize=True)        \n",
    "        audio           = whisper.pad_or_trim(audio.flatten())\n",
    "        clean_mels      = whisper.log_mel_spectrogram(audio).squeeze()        \n",
    "        \n",
    "      \n",
    "        return {\"clean_mels\":     clean_mels.unsqueeze(dim=0),   \n",
    "                \"noise_mels\":     noise_mels.unsqueeze(dim=0), \n",
    "                \"sentance\":       sentance}\n",
    "        \n",
    "\n",
    "\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    def __call__(self, features):\n",
    "                    \n",
    "        clean_mels, noise_mels, sentance = [], [], []\n",
    "        for f in features:\n",
    "            clean_mels           .append(f[\"clean_mels\"])            \n",
    "            noise_mels           .append(f[\"noise_mels\"])            \n",
    "            sentance             .append(f[\"sentance\"])    \n",
    "\n",
    "        clean_mels = torch.concat([mel[None, :] for mel in clean_mels])\n",
    "        noise_mels = torch.concat([mel[None, :] for mel in noise_mels])\n",
    "        \n",
    "            \n",
    "        batch = {\n",
    "            \"sentance\":               sentance,\n",
    "            \"clean_mels\":             clean_mels,\n",
    "            \"noise_mels\":             noise_mels,            \n",
    "        }\n",
    "     \n",
    "        return batch\n",
    "\n",
    "\n",
    "train_dataset    = AudioDataset(train_df)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size = BATCH_SIZE, \n",
    "                                               drop_last  = True, \n",
    "                                               shuffle    = True,\n",
    "                                               collate_fn = WhisperDataCollatorWithPadding())\n",
    "\n",
    "val_dataset      = AudioDataset(val_df)\n",
    "val_dataloader   = torch.utils.data.DataLoader(val_dataset, \n",
    "                                               batch_size = BATCH_SIZE, \n",
    "                                               drop_last  = True, \n",
    "                                               shuffle    = True,\n",
    "                                               collate_fn = WhisperDataCollatorWithPadding())\n",
    "\n",
    "\n",
    "#res = next(iter(train_dataloader))\n",
    "#type(res), res.keys(), res['clean_mels'].shape, res['noise_mels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081fb16-85bc-4f77-887b-c56bb1131113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5b2a0d4",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='unet_model'> <center> Unet Model: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa246d",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc76a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\" A single Res-Block module \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, use_bias: bool):\n",
    "        \"\"\"\n",
    "        Init\n",
    "        :param dim: The dimension\n",
    "        :param use_bias: Flag to use bias or not\n",
    "        \"\"\"\n",
    "        super(ResnetBlock, self).__init__()\n",
    "\n",
    "        # A res-block without the skip-connection, pad-conv-norm-relu-pad-conv-norm\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(dim, dim // 4, kernel_size=1, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(dim // 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(dim // 4, dim // 4, kernel_size=3, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(dim // 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(dim // 4, dim, kernel_size=1, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Feed forward run\n",
    "        :param input_tensor: The input tensor\n",
    "        :return: The output tensor\n",
    "        \"\"\"\n",
    "        # The skip connection is applied here\n",
    "        return input_tensor + self.conv_block(input_tensor)\n",
    "class RescaleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Rescale Block class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers: int, scale: Optional[float] = 0.5, n_mels: Optional[int] = 64,\n",
    "                 use_bias: Optional[bool] = True):\n",
    "        \"\"\"\n",
    "        Init\n",
    "        :param n_layers: The number of layers\n",
    "        :param scale: Scale factor\n",
    "        :param n_mels: Base number of channels\n",
    "        :param use_bias: Flag to use bias or not\n",
    "        \"\"\"\n",
    "        super(RescaleBlock, self).__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.conv_layers = [None] * n_layers\n",
    "\n",
    "        in_channel_power = scale > 1\n",
    "        out_channel_power = scale < 1\n",
    "        i_range = range(n_layers) if scale < 1 else range(n_layers - 1, -1, -1)\n",
    "\n",
    "        for i in i_range:\n",
    "            self.conv_layers[i] = nn.Sequential(\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=n_mels * 2 ** (i + in_channel_power),\n",
    "                        out_channels=n_mels * 2 ** (i + out_channel_power),\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        bias=use_bias,\n",
    "                    )\n",
    "                ),\n",
    "                nn.BatchNorm2d(n_mels * 2 ** (i + out_channel_power)),\n",
    "                nn.LeakyReLU(0.2, True))\n",
    "\n",
    "            self.add_module(\"conv_%d\" % i, self.conv_layers[i])\n",
    "\n",
    "        if scale > 1:\n",
    "            self.conv_layers = self.conv_layers[::-1]\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor,\n",
    "                pyramid: Optional[torch.Tensor] = None,\n",
    "                return_all_scales: Optional[bool] = False,\n",
    "                skip: Optional[bool] = False) -> Tuple[torch.Tensor, Optional[List[torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_tensor: The input tensor\n",
    "        :param pyramid: The pyramid tensor\n",
    "        :param return_all_scales: Flag to return all scales\n",
    "        :param skip: Flag to skip or not\n",
    "        :return: Tuple with feature maps and all scales (if return_all_scales is True)\n",
    "        \"\"\"\n",
    "        feature_map = input_tensor\n",
    "        all_scales = []\n",
    "        if return_all_scales:\n",
    "            all_scales.append(feature_map)\n",
    "\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "\n",
    "            if self.scale > 1.0:\n",
    "                feature_map = f.interpolate(\n",
    "                    feature_map, scale_factor=self.scale, mode=\"nearest\"\n",
    "                )\n",
    "\n",
    "            feature_map = conv_layer(feature_map)\n",
    "\n",
    "            if skip:\n",
    "                feature_map = feature_map + pyramid[-i - 2]\n",
    "\n",
    "            if self.scale < 1.0:\n",
    "                feature_map = self.max_pool(feature_map)\n",
    "\n",
    "            if return_all_scales:\n",
    "                all_scales.append(feature_map)\n",
    "\n",
    "        return (feature_map, all_scales) if return_all_scales else (feature_map, None)\n",
    "    \n",
    "class Unet(nn.Module):\n",
    "    \"\"\" Architecture of the Unet, uses res-blocks \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_mels: Optional[int] = 64,\n",
    "            n_blocks: Optional[int] = 6,\n",
    "            n_downsampling: Optional[int] = 3,\n",
    "            use_bias: Optional[bool] = True,\n",
    "            skip_flag: Optional[bool] = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Init\n",
    "        :param n_mels: The base number of channels\n",
    "        :param n_blocks: The number of res blocks\n",
    "        :param n_downsampling: The number of downsampling blocks\n",
    "        :param use_bias: Use bias or not\n",
    "        :param skip_flag: Use skip connections or not\n",
    "        \"\"\"\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        # Determine whether to use skip connections\n",
    "        self.skip = skip_flag\n",
    "\n",
    "        # Entry block\n",
    "        # First conv-block, no stride so image dims are kept and channels dim is expanded (pad-conv-norm-relu)\n",
    "        self.entry_block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(1, n_mels, kernel_size=7, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(n_mels),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        # Downscaling\n",
    "        # A sequence of strided conv-blocks. Image dims shrink by 2, channels dim expands by 2 at each block\n",
    "        self.downscale_block = RescaleBlock(n_downsampling, 0.5, n_mels, True)\n",
    "\n",
    "        # Bottleneck\n",
    "        # A sequence of res-blocks\n",
    "        bottleneck_block = []\n",
    "        for _ in range(n_blocks):\n",
    "            # noinspection PyUnboundLocalVariable\n",
    "            bottleneck_block += [\n",
    "                ResnetBlock(n_mels * 2 ** n_downsampling, use_bias=use_bias)\n",
    "            ]\n",
    "        self.bottleneck_block = nn.Sequential(*bottleneck_block)\n",
    "\n",
    "        # Upscaling\n",
    "        # A sequence of transposed-conv-blocks, Image dims expand by 2, channels dim shrinks by 2 at each block\\\n",
    "        self.upscale_block = RescaleBlock(n_downsampling, 2.0, n_mels, True)\n",
    "\n",
    "        # Final block\n",
    "        # No stride so image dims are kept and channels dim shrinks to 3 (output image channels)\n",
    "        self.final_block = nn.Sequential(\n",
    "            # nn.ReflectionPad2d(3), nn.Conv2d(n_mels, 1, kernel_size=7), nn.Tanh()\n",
    "            # TODO: without Tanh, for not having output [-1,1]\n",
    "            nn.ReflectionPad2d(3), nn.Conv2d(n_mels, 1, kernel_size=7)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Feed forward run\n",
    "        :param input_tensor: The input Tensor\n",
    "        :param output_size: The output size\n",
    "        :param random_affine: List of random affine numbers\n",
    "        :return: The output tensor\n",
    "        \"\"\"\n",
    "        # A condition for having the output at same size as the scaled input is having even output_size\n",
    "\n",
    "        # Entry block\n",
    "        feature_map = self.entry_block(input_tensor)\n",
    "\n",
    "        # Downscale block\n",
    "        feature_map, downscales = self.downscale_block(\n",
    "            feature_map, return_all_scales=self.skip\n",
    "        )\n",
    "\n",
    "        # Bottleneck (res-blocks)\n",
    "        feature_map = self.bottleneck_block(feature_map)\n",
    "\n",
    "        # Upscale block\n",
    "        feature_map, _ = self.upscale_block(\n",
    "            feature_map, pyramid=downscales, skip=self.skip\n",
    "        )\n",
    "\n",
    "        # Final block\n",
    "        output_tensor = self.final_block(feature_map)\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        cuda = True\n",
    "        state = { 'net': self.state_dict() if cuda else self.state_dict() }\n",
    "\n",
    "        torch.save(state,  model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f7c6c",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='whisper_denoiser'> <center> Whisper Denoiser: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c57418",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetWhisperModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        lang                 = \"ru\"        \n",
    "        self.loss_ce         = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.l1loss          = torch.nn.L1Loss()\n",
    "        self.metrics_wer     = evaluate.load(\"wer\")\n",
    "        \n",
    "        self.whisper_options = whisper.DecodingOptions(language=lang, without_timestamps=True)\n",
    "        self.whisper_model   = whisper.load_model(WHISPER_TYPE)\n",
    "      \n",
    "        config                   = {}\n",
    "        config[\"n_mels\"]         = 80\n",
    "        config[\"n_blocks\"]       = 6\n",
    "        config[\"n_downsampling\"] = 3\n",
    "        config[\"use_bias\"]       = True\n",
    "        config[\"skip_flag\"]      = True\n",
    "        self.unet                = Unet(config[\"n_mels\"], config[\"n_blocks\"], config[\"n_downsampling\"], config[\"use_bias\"], config[\"skip_flag\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        print(\"\\n\")\n",
    "        print(\"--> forward\")\n",
    "        logits       = self.unet(x)        \n",
    "        whisper_pred = self.whisper(logits)        \n",
    "        \n",
    "        return logits, whisper_pred\n",
    "\n",
    "    def calc_wer_and_loss(self, batch):\n",
    "        clean_mels          = batch['clean_mels']        \n",
    "        noise_mels          = batch['noise_mels']        \n",
    "        gt_sentance         = batch['sentance']    \n",
    "        \n",
    "        logits              = self.unet(noise_mels)              \n",
    "        l_whisper_result    = whisper.decode(self.whisper_model, logits.squeeze(), self.whisper_options)        \n",
    "\n",
    "        l_predict_text      = []\n",
    "        for whisper_result in l_whisper_result:\n",
    "            l_predict_text.append(whisper_result.text)\n",
    "       \n",
    "        loss                = self.l1loss(logits, clean_mels)       \n",
    "        wer                 = self.metrics_wer.compute(references=gt_sentance, predictions=l_predict_text)\n",
    "\n",
    "        return loss, wer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        loss, wer = self.calc_wer_and_loss(batch)    \n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train/wer\", wer, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\":loss}\n",
    "\n",
    "\n",
    "    def eval_step(self, batch):\n",
    "        loss, wer = self.calc_wer_and_loss(batch)            \n",
    "        self.log(\"val/loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/wer\",  wer,  on_epoch=True, prog_bar=True, logger=True)                    \n",
    "        return {\"loss\": loss, \"wer\": wer}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):          \n",
    "        return self.eval_step(batch)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):        \n",
    "        return self.eval_step(batch)\n",
    "        \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        optimizer  = torch.optim.Adam(parameters, lr=0.0001)        \n",
    "        return {\n",
    "                \"optimizer\": optimizer,\n",
    "                }\n",
    "\n",
    "        \n",
    "solver = UnetWhisperModel()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0446a-a6a6-45b5-8a5f-51117f311a29",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#16755D;\"> <a id='Debug_Code'> <center> Debug Code: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d80cc-c8a1-467f-a722-b88104e88678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30adb293-203f-4399-a1dc-f6929832e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_CODE = False\n",
    "if DEBUG_CODE is True:\n",
    "\n",
    "    # --- define unet\n",
    "    config                   = {}\n",
    "    config[\"n_mels\"]         = 80\n",
    "    config[\"n_blocks\"]       = 6\n",
    "    config[\"n_downsampling\"] = 3\n",
    "    config[\"use_bias\"]       = True\n",
    "    config[\"skip_flag\"]      = True\n",
    "    unet                     = Unet(config[\"n_mels\"], config[\"n_blocks\"], config[\"n_downsampling\"], config[\"use_bias\"], config[\"skip_flag\"])\n",
    "\n",
    "    # --- get batch\n",
    "    batch                   = next(iter(train_dataloader))\n",
    "    clean_mels              = batch['clean_mels']        \n",
    "    noise_mels              = batch['noise_mels']   \n",
    "    gt_sentance             = batch['sentance']        \n",
    "\n",
    "    # run unet\n",
    "    logits                  = unet(noise_mels)              \n",
    "\n",
    "    # calc loss (l1 loss)\n",
    "    l1loss                 = torch.nn.L1Loss()\n",
    "    loss                   = l1loss(logits, clean_mels)       \n",
    "    print(f\"loss = {loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60118c0",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='train'> <center> Train: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4f93d",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tb_logger    = TensorBoardLogger(save_dir=\"logs/\")\n",
    "wandb_logger = WandbLogger(project=\"UnetWhisperModel\") \n",
    "trainer      = pl.Trainer(max_epochs              = 50,\n",
    "                          logger                  = wandb_logger,\n",
    "                          accumulate_grad_batches = 4)\n",
    "\n",
    "trainer.fit(solver, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa752d-7e24-4c3e-b71b-518cffe3f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext tensorboard\n",
    "#%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce8b01-ba9d-477f-bf2f-1dc053cda87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 64 ->runtimeerror input tensor must fit into 32-bit index math\n",
    "# batch_size = 32 -> out of memoty\n",
    "# bafore 1229MiB / 49140MiB \n",
    "# babtch size = 16 ( 47915MiB / 49140MiB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895bd41",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
