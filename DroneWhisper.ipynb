{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e615cd2",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid green; padding: 20px;\">\n",
    "  <p style='color:green'> Notes & Qs:\n",
    "      <li style='color:green'> Batch size 16 (with tiny whisper) - 47/48G </li>            \n",
    "      <li style='color:green'> Batch size 8 (with medium whisper) - 28/48G </li>            \n",
    "      <li style='color:green'> why there are 2 loss ? (l1 and cross-entory) ? </li>            \n",
    "  </p>  \n",
    "  <p style='color:red'> Todo:\n",
    "      <li style='color:red'> Add normalizer (ru norm) </li>                        \n",
    "  </p>  \n",
    "  <p style='color:blue'> Conclusions & Notes:      \n",
    "      <li style='color:blue'> ? </li>      \n",
    "  </p>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f40aa-2767-4eda-ba38-a2bce698379e",
   "metadata": {},
   "source": [
    "![title](\"./full_arc.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbc6fb-cd36-4196-8e7b-e52541f97815",
   "metadata": {},
   "source": [
    "![title](\"img/picture.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec701f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch           import Trainer\n",
    "from typing                      import Optional, List, Tuple, Union\n",
    "from whisper.tokenizer           import get_tokenizer\n",
    "from lightning.pytorch           import Trainer\n",
    "from torch.nn.functional         import pad\n",
    "from pytorch_lightning.loggers   import TensorBoardLogger\n",
    "from pytorch_lightning.loggers   import WandbLogger\n",
    "from sklearn.model_selection     import train_test_split\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from tqdm                        import tqdm\n",
    "\n",
    "import torchaudio.transforms     as at\n",
    "import torch.nn                  as nn\n",
    "import pandas                    as pd\n",
    "import numpy                     as np\n",
    "import lightning                 as pl\n",
    "import torch.nn.functional       as f\n",
    "\n",
    "import whisper\n",
    "import evaluate\n",
    "import torch\n",
    "import torchaudio\n",
    "import glob\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e332c22-aad4-409e-ac89-32bf6f6f571d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE    = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a9851",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='toc'> <center> Table Of Content: </center> </a>  </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f1010",
   "metadata": {},
   "source": [
    "[DATASET](#DATASET) <br>\n",
    "[UNET Model](#unet_model) <br>\n",
    "[Whisper Denoiser](#whisper_denoiser) <br>\n",
    "[Train](#train) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53b34c-cbab-4d94-b6be-8e34dc9939a8",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:red;\"> <a id='params'> <center> PARAMS: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7c422a-29ee-4adb-a5ff-7d8df6dfef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_TYPE = \"medium\"\n",
    "LANGUAGE     = \"ru\"\n",
    "BATCH_SIZE   = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc796974",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='DATASET'> <center> DATASET: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b908c6",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023301de-06d8-4714-8f5a-0a16c01ddae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df                = pd.read_csv(\"noised_files_v2.csv\")\n",
    "train_df, temp_df = train_test_split(df,      test_size=0.3, random_state=42)\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2bb6fe-e297-4db5-99ee-0c6fc45c2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df), len(val_df) # (3500, 1005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "627c166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_data, language):\n",
    "        \n",
    "        self.df                     = df_data          \n",
    "        self.woptions               = whisper.DecodingOptions(language=language, without_timestamps=True)        \n",
    "        self.multilingual_tokenizer = get_tokenizer(multilingual=True, language=language, task=self.woptions.task)\n",
    "\n",
    "        \n",
    "    def get_sentance(self, fname):                \n",
    "        fname    = fname[fname.rfind('/') + 1 : ]        \n",
    "        fname    = fname.replace(\"wav\", \"mp3\")        \n",
    "        sentence = self.df[self.df.file == fname]['sentence'].values[0]             \n",
    "        \n",
    "        return sentence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, i):               \n",
    "        fname               = self.df.file.values[i]  \n",
    "        sentance            = self.get_sentance(fname)        \n",
    "        \n",
    "        noise_file_name     = rf\"Noised_Dataset/{fname.replace('mp3', 'wav')}\"                     \n",
    "        audio, sr           = torchaudio.load(noise_file_name, normalize=True)        \n",
    "        audio               = whisper.pad_or_trim(audio.flatten())        \n",
    "        noise_mels          = whisper.log_mel_spectrogram(audio).squeeze()                \n",
    "\n",
    "        clean_file_name     = rf\"Clean_Dataset/{fname.replace('mp3', 'wav')}\"          \n",
    "        audio, sr           = torchaudio.load(clean_file_name, normalize=True)        \n",
    "        audio               = whisper.pad_or_trim(audio.flatten())\n",
    "        clean_mels          = whisper.log_mel_spectrogram(audio).squeeze()        \n",
    "\n",
    "        # --- get tokens (will be used for ce loss)\n",
    "        multilingual_tokens = [*self.multilingual_tokenizer.sot_sequence_including_notimestamps] + self.multilingual_tokenizer.encode(sentance)\n",
    "        gt_tokens           = multilingual_tokens[1:] + [self.multilingual_tokenizer.eot]\n",
    "              \n",
    "        return {\"file_name\"          :fname,\n",
    "                \"clean_mels\"         : clean_mels.unsqueeze(dim=0),   \n",
    "                \"noise_mels\"         : noise_mels.unsqueeze(dim=0), \n",
    "                \"sentance\"           : sentance,\n",
    "                \"multilingual_tokens\": multilingual_tokens,\n",
    "                \"gt_tokens\"          : gt_tokens}\n",
    "        \n",
    "\n",
    "\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    def __call__(self, features):\n",
    "        \n",
    "        file_name, clean_mels, noise_mels, sentance, multilingual_tokens, gt_tokens = [], [], [], [], [], []\n",
    "        for f in features:\n",
    "            file_name            .append(f[\"file_name\"])\n",
    "            clean_mels           .append(f[\"clean_mels\"])            \n",
    "            noise_mels           .append(f[\"noise_mels\"])            \n",
    "            sentance             .append(f[\"sentance\"]) \n",
    "            multilingual_tokens  .append(f[\"multilingual_tokens\"])\n",
    "            gt_tokens            .append(f[\"gt_tokens\"])\n",
    "\n",
    "        clean_mels = torch.concat([mel[None, :] for mel in clean_mels])\n",
    "        noise_mels = torch.concat([mel[None, :] for mel in noise_mels])\n",
    "\n",
    "        gt_tokens_lengths          = [len(lab) for lab in gt_tokens]\n",
    "        multilingual_tokens_length = [len(e) for e in multilingual_tokens]\n",
    "        max_label_len              = max(gt_tokens_lengths + multilingual_tokens_length)\n",
    "\n",
    "        gt_tokens           = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100)     for lab, lab_len in zip(gt_tokens,           gt_tokens_lengths)]\n",
    "        multilingual_tokens = [np.pad(e,   (0, max_label_len - e_len),   'constant', constant_values=50257)    for e, e_len     in zip(multilingual_tokens, multilingual_tokens_length)] \n",
    "                        \n",
    "        batch = {            \n",
    "            \"multilingual_tokens\":    multilingual_tokens,\n",
    "            \"gt_tokens\"          :    gt_tokens \n",
    "        }\n",
    "\n",
    "        batch               = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
    "        batch[\"file_name\"]  = file_name\n",
    "        batch[\"sentance\"]   = sentance\n",
    "        batch[\"clean_mels\"] = clean_mels\n",
    "        batch[\"noise_mels\"] = noise_mels\n",
    "                                    \n",
    "        return batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cd60307-b2a4-4994-8c8c-8889cddc6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset    = AudioDataset(train_df, LANGUAGE)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size = BATCH_SIZE, \n",
    "                                               drop_last  = True, \n",
    "                                               shuffle    = True,\n",
    "                                               collate_fn = WhisperDataCollatorWithPadding())\n",
    "\n",
    "val_dataset      = AudioDataset(val_df, LANGUAGE)\n",
    "val_dataloader   = torch.utils.data.DataLoader(val_dataset, \n",
    "                                               batch_size = BATCH_SIZE, \n",
    "                                               drop_last  = True, \n",
    "                                               shuffle    = True,\n",
    "                                               collate_fn = WhisperDataCollatorWithPadding())\n",
    "\n",
    "\n",
    "#res = next(iter(train_dataloader))\n",
    "#type(res['multilingual_tokens'][0])\n",
    "#type(res), res.keys(), res['clean_mels'].shape, res['noise_mels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081fb16-85bc-4f77-887b-c56bb1131113",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset     = AudioDataset(test_df, LANGUAGE)\n",
    "test_dataloader  = torch.utils.data.DataLoader(test_dataset, \n",
    "                                               batch_size = BATCH_SIZE, \n",
    "                                               drop_last  = True, \n",
    "                                               shuffle    = True,\n",
    "                                               collate_fn = WhisperDataCollatorWithPadding())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2a0d4",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='unet_model'> <center> Unet Model: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa246d",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc76a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\" A single Res-Block module \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, use_bias: bool):\n",
    "        \"\"\"\n",
    "        Init\n",
    "        :param dim: The dimension\n",
    "        :param use_bias: Flag to use bias or not\n",
    "        \"\"\"\n",
    "        super(ResnetBlock, self).__init__()\n",
    "\n",
    "        # A res-block without the skip-connection, pad-conv-norm-relu-pad-conv-norm\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(dim, dim // 4, kernel_size=1, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(dim // 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(dim // 4, dim // 4, kernel_size=3, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(dim // 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(dim // 4, dim, kernel_size=1, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Feed forward run\n",
    "        :param input_tensor: The input tensor\n",
    "        :return: The output tensor\n",
    "        \"\"\"\n",
    "        # The skip connection is applied here\n",
    "        return input_tensor + self.conv_block(input_tensor)\n",
    "class RescaleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Rescale Block class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers: int, scale: Optional[float] = 0.5, n_mels: Optional[int] = 64,\n",
    "                 use_bias: Optional[bool] = True):\n",
    "        \"\"\"\n",
    "        Init\n",
    "        :param n_layers: The number of layers\n",
    "        :param scale: Scale factor\n",
    "        :param n_mels: Base number of channels\n",
    "        :param use_bias: Flag to use bias or not\n",
    "        \"\"\"\n",
    "        super(RescaleBlock, self).__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.conv_layers = [None] * n_layers\n",
    "\n",
    "        in_channel_power = scale > 1\n",
    "        out_channel_power = scale < 1\n",
    "        i_range = range(n_layers) if scale < 1 else range(n_layers - 1, -1, -1)\n",
    "\n",
    "        for i in i_range:\n",
    "            self.conv_layers[i] = nn.Sequential(\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=n_mels * 2 ** (i + in_channel_power),\n",
    "                        out_channels=n_mels * 2 ** (i + out_channel_power),\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        bias=use_bias,\n",
    "                    )\n",
    "                ),\n",
    "                nn.BatchNorm2d(n_mels * 2 ** (i + out_channel_power)),\n",
    "                nn.LeakyReLU(0.2, True))\n",
    "\n",
    "            self.add_module(\"conv_%d\" % i, self.conv_layers[i])\n",
    "\n",
    "        if scale > 1:\n",
    "            self.conv_layers = self.conv_layers[::-1]\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor,\n",
    "                pyramid: Optional[torch.Tensor] = None,\n",
    "                return_all_scales: Optional[bool] = False,\n",
    "                skip: Optional[bool] = False) -> Tuple[torch.Tensor, Optional[List[torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_tensor: The input tensor\n",
    "        :param pyramid: The pyramid tensor\n",
    "        :param return_all_scales: Flag to return all scales\n",
    "        :param skip: Flag to skip or not\n",
    "        :return: Tuple with feature maps and all scales (if return_all_scales is True)\n",
    "        \"\"\"\n",
    "        feature_map = input_tensor\n",
    "        all_scales = []\n",
    "        if return_all_scales:\n",
    "            all_scales.append(feature_map)\n",
    "\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "\n",
    "            if self.scale > 1.0:\n",
    "                feature_map = f.interpolate(\n",
    "                    feature_map, scale_factor=self.scale, mode=\"nearest\"\n",
    "                )\n",
    "\n",
    "            feature_map = conv_layer(feature_map)\n",
    "\n",
    "            if skip:\n",
    "                feature_map = feature_map + pyramid[-i - 2]\n",
    "\n",
    "            if self.scale < 1.0:\n",
    "                feature_map = self.max_pool(feature_map)\n",
    "\n",
    "            if return_all_scales:\n",
    "                all_scales.append(feature_map)\n",
    "\n",
    "        return (feature_map, all_scales) if return_all_scales else (feature_map, None)\n",
    "    \n",
    "class Unet(nn.Module):\n",
    "    \"\"\" Architecture of the Unet, uses res-blocks \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_mels: Optional[int] = 64,\n",
    "            n_blocks: Optional[int] = 6,\n",
    "            n_downsampling: Optional[int] = 3,\n",
    "            use_bias: Optional[bool] = True,\n",
    "            skip_flag: Optional[bool] = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Init\n",
    "        :param n_mels: The base number of channels\n",
    "        :param n_blocks: The number of res blocks\n",
    "        :param n_downsampling: The number of downsampling blocks\n",
    "        :param use_bias: Use bias or not\n",
    "        :param skip_flag: Use skip connections or not\n",
    "        \"\"\"\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        # Determine whether to use skip connections\n",
    "        self.skip = skip_flag\n",
    "\n",
    "        # Entry block\n",
    "        # First conv-block, no stride so image dims are kept and channels dim is expanded (pad-conv-norm-relu)\n",
    "        self.entry_block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(1, n_mels, kernel_size=7, bias=use_bias)\n",
    "            ),\n",
    "            nn.BatchNorm2d(n_mels),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        # Downscaling\n",
    "        # A sequence of strided conv-blocks. Image dims shrink by 2, channels dim expands by 2 at each block\n",
    "        self.downscale_block = RescaleBlock(n_downsampling, 0.5, n_mels, True)\n",
    "\n",
    "        # Bottleneck\n",
    "        # A sequence of res-blocks\n",
    "        bottleneck_block = []\n",
    "        for _ in range(n_blocks):\n",
    "            # noinspection PyUnboundLocalVariable\n",
    "            bottleneck_block += [\n",
    "                ResnetBlock(n_mels * 2 ** n_downsampling, use_bias=use_bias)\n",
    "            ]\n",
    "        self.bottleneck_block = nn.Sequential(*bottleneck_block)\n",
    "\n",
    "        # Upscaling\n",
    "        # A sequence of transposed-conv-blocks, Image dims expand by 2, channels dim shrinks by 2 at each block\\\n",
    "        self.upscale_block = RescaleBlock(n_downsampling, 2.0, n_mels, True)\n",
    "\n",
    "        # Final block\n",
    "        # No stride so image dims are kept and channels dim shrinks to 3 (output image channels)\n",
    "        self.final_block = nn.Sequential(\n",
    "            # nn.ReflectionPad2d(3), nn.Conv2d(n_mels, 1, kernel_size=7), nn.Tanh()\n",
    "            # TODO: without Tanh, for not having output [-1,1]\n",
    "            nn.ReflectionPad2d(3), nn.Conv2d(n_mels, 1, kernel_size=7)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Feed forward run\n",
    "        :param input_tensor: The input Tensor\n",
    "        :param output_size: The output size\n",
    "        :param random_affine: List of random affine numbers\n",
    "        :return: The output tensor\n",
    "        \"\"\"\n",
    "        # A condition for having the output at same size as the scaled input is having even output_size\n",
    "\n",
    "        # Entry block\n",
    "        feature_map = self.entry_block(input_tensor)\n",
    "\n",
    "        # Downscale block\n",
    "        feature_map, downscales = self.downscale_block(\n",
    "            feature_map, return_all_scales=self.skip\n",
    "        )\n",
    "\n",
    "        # Bottleneck (res-blocks)\n",
    "        feature_map = self.bottleneck_block(feature_map)\n",
    "\n",
    "        # Upscale block\n",
    "        feature_map, _ = self.upscale_block(\n",
    "            feature_map, pyramid=downscales, skip=self.skip\n",
    "        )\n",
    "\n",
    "        # Final block\n",
    "        output_tensor = self.final_block(feature_map)\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        cuda  = True\n",
    "        state = { 'net': self.state_dict() if cuda else self.state_dict() }\n",
    "\n",
    "        torch.save(state,  model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f7c6c",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='whisper_denoiser'> <center> Whisper Denoiser: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c57418",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fa874-cb77-445e-8e00-244e63910813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ef874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetWhisperModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        lang                 = LANGUAGE\n",
    "        self.loss_ce         = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.l1loss          = torch.nn.L1Loss()\n",
    "        self.metrics_wer     = evaluate.load(\"wer\")\n",
    "        \n",
    "        self.whisper_options = whisper.DecodingOptions(language=lang, without_timestamps=True)\n",
    "        self.whisper_model   = whisper.load_model(WHISPER_TYPE)\n",
    "        self.tokenizer       = whisper.tokenizer.get_tokenizer(True, language=LANGUAGE, task=self.whisper_options.task)\n",
    "\n",
    "        for param in self.whisper_model.parameters():\n",
    "            param.requires_grad = False\n",
    "  \n",
    "        config                   = {}\n",
    "        config[\"n_mels\"]         = 80\n",
    "        config[\"n_blocks\"]       = 6\n",
    "        config[\"n_downsampling\"] = 3\n",
    "        config[\"use_bias\"]       = True\n",
    "        config[\"skip_flag\"]      = True\n",
    "        self.unet                = Unet(config[\"n_mels\"], config[\"n_blocks\"], config[\"n_downsampling\"], config[\"use_bias\"], config[\"skip_flag\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        print(\"\\n\")\n",
    "        print(\"--> forward\")\n",
    "        logits       = self.unet(x)        \n",
    "        whisper_pred = self.whisper(logits)        \n",
    "        \n",
    "        return logits, whisper_pred\n",
    "\n",
    "    def calc_loss(self, batch):\n",
    "                \n",
    "        clean_mels          = batch['clean_mels']        \n",
    "        noise_mels          = batch['noise_mels']        \n",
    "        gt_sentance         = batch['sentance']    \n",
    "        multilingual_tokens = batch['multilingual_tokens'].long()\n",
    "        gt_tokens           = batch['gt_tokens'].long() \n",
    "        \n",
    "        logits              = self.unet(noise_mels)                      \n",
    "        audio_features      = self.whisper_model.encoder(logits.squeeze())        \n",
    "        out                 = self.whisper_model.decoder(multilingual_tokens, audio_features)        \n",
    "        \n",
    "        # --- calc loss        \n",
    "        l1_loss              = self.l1loss(logits, clean_mels)                           \n",
    "        ce_loss              = self.loss_ce(out.view(-1, out.size(-1)), gt_tokens.view(-1))                                        \n",
    "        loss                 = l1_loss * 0.5 + ce_loss * 0.5\n",
    "                    \n",
    "        return loss, out, gt_tokens\n",
    "\n",
    "    def calc_wer(self, whisper_out, gt_tokens):\n",
    "        whisper_out[whisper_out == -100]         = self.tokenizer.eot\n",
    "        gt_tokens  [gt_tokens == -100]           = self.tokenizer.eot\n",
    "        o_list, l_list                           = [], []\n",
    "        \n",
    "        for o, l in zip(whisper_out, gt_tokens):\n",
    "            o = torch.argmax(o, dim=1)\n",
    "            o_list.append(self.tokenizer.decode(o).lower())\n",
    "            l_list.append(self.tokenizer.decode(l).lower().replace('<|endoftext|>', '').replace(\"<|en|>\", \"\").replace(\"<|transcribe|>\", \"\").replace(\"<|notimestamps|>\", \"\"))                \n",
    "        wer = self.metrics_wer.compute(references=l_list, predictions=o_list)\n",
    "        \n",
    "        return wer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):        \n",
    "        loss, whisper_out, gt_tokens = self.calc_loss(batch)    \n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)        \n",
    "        return {\"loss\":loss}\n",
    "\n",
    "\n",
    "    def eval_step(self, batch):        \n",
    "        loss, whisper_out, gt_tokens = self.calc_loss(batch)            \n",
    "        wer                          = self.calc_wer(whisper_out, gt_tokens)\n",
    "        self.log(\"val/loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/wer\",  wer,  on_epoch=True, prog_bar=True, logger=True)                    \n",
    "        return {\"loss\": loss, \"wer\": wer}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):                  \n",
    "        loss, whisper_out, gt_tokens = self.calc_loss(batch)            \n",
    "        wer                          = self.calc_wer(whisper_out, gt_tokens)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_wer\",  wer,  on_epoch=True, prog_bar=True, logger=True)                            \n",
    "        return {\"val_loss\": loss, \"val_wer\": wer}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):         \n",
    "        loss, whisper_out, gt_tokens = self.calc_loss(batch)            \n",
    "        wer                          = self.calc_wer(whisper_out, gt_tokens)    \n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_wer\",  wer,  on_epoch=True, prog_bar=True, logger=True)                            \n",
    "        return {\"test_loss\": loss, \"test_wer\": wer}\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        optimizer  = torch.optim.Adam(parameters, lr=0.0001)        \n",
    "        return {\n",
    "                \"optimizer\": optimizer\n",
    "                }\n",
    "\n",
    "\n",
    "solver = UnetWhisperModel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b184c69-77d7-4ef1-a8e5-e777d68397f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0446a-a6a6-45b5-8a5f-51117f311a29",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#16755D;\"> <a id='Debug_Code'> <center> Debug Code: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d80cc-c8a1-467f-a722-b88104e88678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30adb293-203f-4399-a1dc-f6929832e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_CODE = False\n",
    "if DEBUG_CODE is True:\n",
    "\n",
    "    # --- define unet\n",
    "    config                   = {}\n",
    "    config[\"n_mels\"]         = 80\n",
    "    config[\"n_blocks\"]       = 6\n",
    "    config[\"n_downsampling\"] = 3\n",
    "    config[\"use_bias\"]       = True\n",
    "    config[\"skip_flag\"]      = True\n",
    "    unet                     = Unet(config[\"n_mels\"], config[\"n_blocks\"], config[\"n_downsampling\"], config[\"use_bias\"], config[\"skip_flag\"])\n",
    "\n",
    "    # --- get batch\n",
    "    batch                   = next(iter(train_dataloader))\n",
    "    clean_mels              = batch['clean_mels']        \n",
    "    noise_mels              = batch['noise_mels']   \n",
    "    gt_sentance             = batch['sentance']        \n",
    "    multilingual_tokens     = batch['multilingual_tokens'].long()\n",
    "    tokens_with_eot         = batch['tokens_with_eot'].long() \n",
    "        \n",
    "\n",
    "    # run unet    \n",
    "    logits          = unet(noise_mels)         \n",
    "    \n",
    "    # run whisper\n",
    "    whisper_options = whisper.DecodingOptions(language=LANGUAGE, without_timestamps=True)\n",
    "    whisper_model   = whisper.load_model(WHISPER_TYPE)\n",
    "\n",
    "    audio_features  = whisper_model.encoder(logits.squeeze().cuda())\n",
    "    out             = whisper_model.decoder(multilingual_tokens.cuda(), audio_features)\n",
    "\n",
    "    # calc loss\n",
    "    whisper_options = whisper.DecodingOptions(language=LANGUAGE, without_timestamps=True)\n",
    "    tokenizer       = whisper.tokenizer.get_tokenizer(True, language=LANGUAGE, task=whisper_options.task)\n",
    "    loss_ce         = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    l1loss          = torch.nn.L1Loss()\n",
    "    metrics_wer     = evaluate.load(\"wer\")\n",
    "    \n",
    "    l1_loss              = l1loss(logits, clean_mels)           \n",
    "    ce_loss              = loss_ce(out.view(-1, out.size(-1)).cuda(), tokens_with_eot.view(-1).cuda())        \n",
    "    loss                 = l1_loss * 0.5 + ce_loss * 0.5\n",
    "    \n",
    "    # --- calc WER\n",
    "    out[out == -100]                = tokenizer.eot\n",
    "    tokens_with_eot[tokens_with_eot == -100] = tokenizer.eot\n",
    "    o_list, l_list                  = [], []\n",
    "    \n",
    "    for o, l in zip(out, tokens_with_eot):\n",
    "        o = torch.argmax(o, dim=1)\n",
    "        o_list.append(tokenizer.decode(o).lower())\n",
    "        l_list.append(tokenizer.decode(l).lower().replace('<|endoftext|>', '').replace(\"<|en|>\", \"\").replace(\"<|transcribe|>\", \"\").replace(\"<|notimestamps|>\", \"\"))        \n",
    "    wer = metrics_wer.compute(references=l_list, predictions=o_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58441ff-7e11-4b6b-997d-7aec6ff70178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcf74b-4202-45c6-a146-101960c9d1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a60118c0",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='train'> <center> Train: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4f93d",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7defcfc7-7a65-45d2-a487-c4fe92c9406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamitli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vscode_user/VoiceTeam/Amit/wandb/run-20240613_150011-y2lrwk8r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit/runs/y2lrwk8r' target=\"_blank\">fragrant-thunder-13</a></strong> to <a href='https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit' target=\"_blank\">https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit/runs/y2lrwk8r' target=\"_blank\">https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit/runs/y2lrwk8r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/amitli/VoiceTeam-Amit_VoiceTeam_Amit/runs/y2lrwk8r?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f020c190a00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5eb843a-e6f7-425b-beef-0adbddb55e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor    = \"val_wer\",\n",
    "    mode       = \"min\",       \n",
    "    dirpath    = \"checkpoints/\", \n",
    "    filename   = \"best_model-{epoch:02d}-{val_wer:.4f}\",\n",
    "    save_top_k = 1,      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ac254-16f8-4097-9c64-0b7ec4d5d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tb_logger    = TensorBoardLogger(save_dir=\"logs/\")\n",
    "wandb_logger = WandbLogger(project=\"UnetWhisperModel\") \n",
    "trainer      = pl.Trainer(max_epochs              = 50,\n",
    "                          logger                  = wandb_logger,\n",
    "                          accumulate_grad_batches = 8,\n",
    "                          callbacks               = [checkpoint_callback])\n",
    "\n",
    "trainer.fit(solver, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa752d-7e24-4c3e-b71b-518cffe3f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext tensorboard\n",
    "#%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56d906-708e-4ee7-a561-b36bf4509454",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#3cA8EF;\"> <a id='test_results'> <center> Test Results: </center> </a> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1366af0-dcba-44fe-8ff2-5c71697d60c1",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid green; padding: 20px;\">\n",
    "  <p style='color:green'>  Results: \n",
    "      <li style='color:green'> MEDIUM, pure whisper - 'test_base_results.csv' - WER= 0.416</li>                  \n",
    "      <li style='color:red'> MEDIUM, With UNET: - 'test_unet_results.csv' - WER= 0.482 - training was stop after 8 epochs </li>                  \n",
    "  </p>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501f250-66f9-4e0b-9665-3f1f8f6fbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model   = whisper.load_model(WHISPER_TYPE)\n",
    "whisper_options = whisper.DecodingOptions(language=\"ru\", without_timestamps=True)\n",
    "metrics_wer     = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b0cf8-8d29-40fb-9fc7-b45752b9726e",
   "metadata": {},
   "source": [
    "<h2> Base results: </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa6628-f1a2-4846-8614-8b3a0d60e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test_files = []\n",
    "l_results    = []\n",
    "l_gt         = []\n",
    "l_wer        = []\n",
    "\n",
    "for i in tqdm(range(len(test_df))):    \n",
    "    fname     = test_df.file.values[i]\n",
    "    gt        = test_df.sentence.values[i]\n",
    "    \n",
    "    full_name = rf\"Noised_Dataset/{fname.replace('mp3', 'wav')}\" \n",
    "    result    = whisper_model.transcribe(full_name)\n",
    "    result    = result[\"text\"]\n",
    "    wer_res   = metrics_wer.compute(references=[gt], predictions=[result])\n",
    "    \n",
    "    l_test_files .append(fname)    \n",
    "    l_results    .append(result)\n",
    "    l_gt         .append(gt)\n",
    "    l_wer        .append(wer_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2e1c2-efb1-4803-bb12-f079d6d33644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_results = pd.DataFrame({\"test_file\": l_test_files,\n",
    "                               \"prediction\": l_results,\n",
    "                               \"gt\"        : l_gt,\n",
    "                               \"wer\"       : l_wer})\n",
    "\n",
    "df_base_results.to_csv(\"test_base_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033f5c5-4b9e-4e2b-9f32-f94550facade",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_base_results.wer.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47b40d-ff49-42c6-b499-8ef3912681ca",
   "metadata": {},
   "source": [
    "<h2> With UNET: </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1f5a1-0ee4-40d1-80a8-d1f5e14aab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path  = \"checkpoints/best_model-epoch=07-val_wer=0.0000.ckpt\"\n",
    "unetWhisperModel = UnetWhisperModel.load_from_checkpoint(checkpoint_path)\n",
    "unetWhisperModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3622f-2b4b-4b1d-9822-bdfe915b8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test_files = []\n",
    "l_results    = []\n",
    "l_gt         = []\n",
    "l_wer        = []\n",
    "\n",
    "for i in tqdm(range(len(test_df))):    \n",
    "\n",
    "    # --- get file\n",
    "    fname           = test_df.file.values[i]\n",
    "    gt              = test_df.sentence.values[i]\n",
    "\n",
    "\n",
    "    # --- get spect\n",
    "    full_name       = rf\"Noised_Dataset/{fname.replace('mp3', 'wav')}\" \n",
    "    audio, sr       = torchaudio.load(full_name, normalize=True)        \n",
    "    audio           = whisper.pad_or_trim(audio.flatten())        \n",
    "    noise_mels      = whisper.log_mel_spectrogram(audio).squeeze()\n",
    "    noise_mels      = noise_mels.unsqueeze(dim=0) \t\t\n",
    "\n",
    "    # --- run unet\n",
    "    logits          = unetWhisperModel.unet(noise_mels.unsqueeze(1).cuda())\n",
    "    whisper_result  = whisper.decode(whisper_model, logits.squeeze(), whisper_options)        \n",
    "    result          = whisper_result.text\n",
    "\n",
    "    # --- calc WER\n",
    "    wer_res   = metrics_wer.compute(references=[gt], predictions=[result])\n",
    "\n",
    "    # --- add to lists\n",
    "    l_test_files .append(fname)    \n",
    "    l_results    .append(result)\n",
    "    l_gt         .append(gt)\n",
    "    l_wer        .append(wer_res)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80afbcfa-d9df-4342-bec3-31d61d55c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unet_results = pd.DataFrame({\"test_file\": l_test_files,\n",
    "                               \"prediction\": l_results,\n",
    "                               \"gt\"        : l_gt,\n",
    "                               \"wer\"       : l_wer})\n",
    "\n",
    "df_unet_results.to_csv(\"test_unet_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4567b-25be-4e8b-9aab-c51eefa51f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_unet_results.wer.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895bd41",
   "metadata": {},
   "source": [
    "[Table of content](#toc) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67175d-72dd-4803-8324-1ab0b2b7b059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
